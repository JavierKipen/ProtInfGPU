#include "GPUCalcManager.h"
#include <iostream>
#include <vector>
#include <cassert>
#include <limits>
#include <chrono>


using namespace std;

__global__ void PIgXRelThreadPerRead(DeviceData *d_devData);
__global__ void PIgXRelThreadPerReadPerProt(DeviceData *d_devData);
__global__ void invertVect(float * vec,unsigned int len);

//Kernel definitions
__global__ void PIgXRelThreadPerRead(DeviceData *d_devData)
{
    const unsigned int threadId = blockIdx.x*blockDim.x + threadIdx.x;
    float pRemRead,PCurrProtGivenReadRel;
    float * d_TopNFluExpScoresRead, *d_PIgXRelRead;
    unsigned int *d_TopNFluExpIdRead,FluExpIdOffset;
    
    if(threadId<d_devData->nReadsProcess)
    {
        pRemRead=d_devData->d_pRem[threadId];
        d_TopNFluExpScoresRead=&(d_devData->d_TopNFluExpScores[threadId*d_devData->nSparsity]); //Pointing towards current read flu scores
        d_TopNFluExpIdRead=&(d_devData->d_TopNFluExpId[threadId*d_devData->nSparsity]); //Pointing towards current read flu scores ids
        d_PIgXRelRead=&(d_devData->d_MatAux[threadId*d_devData->nProt]); //Pointing towards current read flu scores ids
        
        FluExpIdOffset=0; //Starts from the beggining of the fluexp array
        for(unsigned int currProt=0;currProt<d_devData->nProt;currProt++) //We will calculate P(I=CurrProt|X=readThreadId) (relative)
        {
            PCurrProtGivenReadRel=0;
            for(unsigned int currReadScoreId=0;currReadScoreId<d_devData->nSparsity;currReadScoreId++) //Sparse matrix calculation!
                for(unsigned int currFluExpOfProtId=FluExpIdOffset;currFluExpOfProtId<FluExpIdOffset+(d_devData->d_NFexpForI[currProt]);currFluExpOfProtId++) //For every possible flu exp that the given protein can produce:
                    if(d_TopNFluExpIdRead[currReadScoreId]== d_devData->d_FexpIdForI[currFluExpOfProtId]) //If the score Id could also be generated by the current protein
                        PCurrProtGivenReadRel += (d_TopNFluExpScoresRead[currReadScoreId] * d_devData->d_PFexpForI[currFluExpOfProtId]); //Accumulates prob
            PCurrProtGivenReadRel += pRemRead; //Adding normalizing error
            d_PIgXRelRead[currProt]=PCurrProtGivenReadRel;
            FluExpIdOffset+=d_devData->d_NFexpForI[currProt]; //Offset to get P(Fe|I=Currprot) and its indexes;
        }
    }
}

__global__ void invertVect(float * vec,unsigned int len){
    int tid = blockDim.x * blockIdx.x + threadIdx.x; //thread id;

    /* if valid, squre the array element */
    if (tid < len) 
        vec[tid] = (1/vec[tid]);
}

__global__ void PIgXRelThreadPerReadPerProt(DeviceData *d_devData)
{
    const unsigned long threadId = (unsigned long)blockIdx.x*(unsigned long)blockDim.x + (unsigned long)threadIdx.x;
    
    float pRemRead,PCurrProtGivenReadRel;
    float * d_TopNFluExpScoresRead, *d_PIgXRelRead;
    unsigned int *d_TopNFluExpIdRead,FluExpIdOffset;
    

    if( threadId< ((unsigned long)d_devData->nReadsProcess*(unsigned long)d_devData->nProt) ) //Only threads within the desired range.
    {
        const unsigned int currRead = threadId/d_devData->nProt; //gets currRead and currProt from thread Id (could still have mix within same block, but it is small)
        const unsigned int currProt = threadId%d_devData->nProt;
        
        pRemRead=d_devData->d_pRem[currRead];
        d_TopNFluExpScoresRead=&(d_devData->d_TopNFluExpScores[(unsigned long)currRead*(unsigned long)d_devData->nSparsity]); //Pointing towards current read flu scores
        d_TopNFluExpIdRead=&(d_devData->d_TopNFluExpId[(unsigned long)currRead*(unsigned long)d_devData->nSparsity]); //Pointing towards current read flu scores ids
        d_PIgXRelRead=&(d_devData->d_MatAux[((unsigned long)currRead*(unsigned long)d_devData->nProt)+(unsigned long)currProt]); //Pointing towards the point in the matrix to be calculated.
        
        //Getting the start of the flu exps prob for the curr protein
        FluExpIdOffset=0; //Starts from the beggining of the fluexp array
        for(unsigned int i=0;i<currProt;i++)
            FluExpIdOffset+=d_devData->d_NFexpForI[i];
       

        PCurrProtGivenReadRel=0;
        for(unsigned int currReadScoreId=0;currReadScoreId<d_devData->nSparsity;currReadScoreId++) //Sparse matrix calculation!
            for(unsigned int currFluExpOfProtId=FluExpIdOffset;currFluExpOfProtId<FluExpIdOffset+(d_devData->d_NFexpForI[currProt]);currFluExpOfProtId++) //For every possible flu exp that the given protein can produce:
                if(d_TopNFluExpIdRead[currReadScoreId]== d_devData->d_FexpIdForI[currFluExpOfProtId]) //If the score Id could also be generated by the current protein
                    PCurrProtGivenReadRel += (d_TopNFluExpScoresRead[currReadScoreId] * d_devData->d_PFexpForI[currFluExpOfProtId]); //Accumulates prob
        PCurrProtGivenReadRel += pRemRead; //Adding normalizing error
        *d_PIgXRelRead=PCurrProtGivenReadRel;
    }
}

bool checkNan(float *pArrayGPU, unsigned long len)
{
    bool retVal=false;
    vector<float> auxVec(len,0);
    cudaMemcpy(auxVec.data(), pArrayGPU, sizeof(float)*len, cudaMemcpyDeviceToHost); //The update is contained in the auxiliar vector.
    for(unsigned long i=0;i<len;i++)
    {
        if(isnan(auxVec[i]) || isinf(auxVec[i]))
        {
            cout << "Element " + to_string(i) + " is not a number." << endl;
            retVal=true;
        }    
    }
    return retVal;
}

bool checkZeroRows(float *pMatAux, unsigned long nProt,unsigned long nReads)
{
    bool retVal=false;
    unsigned long len = nReads*nProt;
    vector<float> auxVec(len,0);
    cudaMemcpy(auxVec.data(), pMatAux, sizeof(float)*len, cudaMemcpyDeviceToHost); //The update is contained in the auxiliar vector.
    for(unsigned long i=0;i<nReads;i++)
    {
        bool allReadZeros=true;
        for(unsigned long j=0;j<nProt;j++)
            if(auxVec[i*nProt+j]!=0)
                allReadZeros=false;
        if(allReadZeros)
            cout << "Row number: " << to_string(i) << " had all probs eq to zero."<< endl;
    }
    return retVal;
}

//Class functions

GPUCalcManager::GPUCalcManager()
{
    NThreadsPerBlock=DEFAULT_N_THREADS_PER_BLOCK;
    
    map<unsigned int, unsigned long> aux={
            { 152   ,    1000000 },
            { 50  ,      1000000 },
            { 1000  ,     100000 },
            { 20660  ,     10000 }
        };
    batchingLenForNProt=aux; //Set the batching map, these values were set after trial and error.
    
    minBatchSize = std::numeric_limits<unsigned long>::max();
    for (const auto& pair : batchingLenForNProt) {
        if (pair.second < minBatchSize) {
            minBatchSize = pair.second;
        }
    }
}

unsigned long GPUCalcManager::retrieveBatch(unsigned int nProt) //I made a map for certain protein numbers I tried, but to cover unexpected cases is this function
{
    auto it = batchingLenForNProt.find(nProt);
    if (it != batchingLenForNProt.end()) {
        return it->second;  // Return value if key exists
    }
    // If key doesnt exist, minimum batch size is returned
    return minBatchSize;

}

void GPUCalcManager::init()
{
    cublasCreate(&cuBlasHandle);
    
}


void GPUCalcManager::calculateUpdate(DeviceData *pdevData, DeviceData *d_pdevData) //Whenever data is ready, this process is run to process the data contribution.
{
    this->pdevData=pdevData;
    this->d_pdevData=d_pdevData;
    auto t1 = chrono::high_resolution_clock::now();
    
    calcPRem(); //Gets normalization factor of sparse matrix
    //checkNan(pdevData->d_pRem,pdevData->nReadsProcess);
    
    auto t2 = chrono::high_resolution_clock::now();
    calcPXgIRel(); //PXgIRel is obtained
    auto t3 = chrono::high_resolution_clock::now();
    
    //checkZeroRows(pdevData->d_MatAux, pdevData->nProt,pdevData->nReadsProcess);
    
    //checkNan(pdevData->d_MatAux,pdevData->nReadsProcess*pdevData->nProt);
    calcPXIRel(); //The joint relative matrix is normalized
    //checkNan(pdevData->d_MatAux,pdevData->nReadsProcess*pdevData->nProt);
    PXIRelSumRows(); //The sums over the the rows are calculated for normalization and alpha calc.
    //checkNan(pdevData->d_VecAux,pdevData->nReadsProcess);
    calcAlphas(); //Multiplies the normalized matrix with the 1/sum for normalizations
    //checkNan(pdevData->d_MatAux,pdevData->nReadsProcess*pdevData->nProt);
    sumAlphas(); //Sums all alphas through reads for the updates to p_I estimation!
    //checkNan(pdevData->d_MatAux,pdevData->nReadsProcess*pdevData->nProt);
    auto t4 = chrono::high_resolution_clock::now();
    std::chrono::duration<double, std::milli> ms_total = t4 - t1;
    std::chrono::duration<double, std::milli> ms_kernel = t3 - t2;
    
    //cout << "Calc total time: " << ms_total.count() << "ms. Kernel time: " << ms_kernel.count() <<"ms\n"; //Comment when not timing!
    
}
void GPUCalcManager::sumAlphas()
{

    float alpha,beta;
    unsigned int m;
    cublasOperation_t trans;
    
    //Parameters fixing
    alpha=1;
    trans=CUBLAS_OP_N; //No transpose
    m=pdevData->nProt; //Cublas uses column-major notation, so we using this notation we can do our original operation
    beta=1;
    cudaError_t err= cudaMemset(pdevData->d_VecAux, 0, m * sizeof(float)); //Sets aux vec to zero to continue accumulating sums
    
    
    unsigned long batchSize=retrieveBatch(pdevData->nProt); 
    for (unsigned int i = 0; i < pdevData->nReadsProcess; i += batchSize) {
        unsigned int currentBatchSize = min( (unsigned int)batchSize, pdevData->nReadsProcess - i); // Handle last batch

        cuBlasStatus = cublasSgemv( cuBlasHandle, trans,
                                    m, currentBatchSize,
                                    &alpha,
                                    pdevData->d_MatAux + (unsigned long)i * (unsigned long)m, m,  // Offset matrix by i * m
                                    pdevData->d_ones , 1,         // Offset ones vector
                                    &beta,
                                    pdevData->d_VecAux, 1);

        assert(cuBlasStatus == CUBLAS_STATUS_SUCCESS && "Error in cuBlas calculation!");
    }

}

/* Approach with batching!
    beta=1;
    cudaError_t err= cudaMemset(pdevData->d_VecAux, 0, m * sizeof(float)); //Sets aux vec to zero to continue accumulating sums
    
    
    unsigned long batchSize=retrieveBatch(pdevData->nProt); 
    for (unsigned int i = 0; i < pdevData->nReadsProcess; i += batchSize) {
        unsigned int currentBatchSize = min( (unsigned int)batchSize, pdevData->nReadsProcess - i); // Handle last batch

        cuBlasStatus = cublasSgemv( cuBlasHandle, trans,
                                    m, currentBatchSize,
                                    &alpha,
                                    pdevData->d_MatAux + i * m, m,  // Offset matrix by i * m
                                    pdevData->d_ones , 1,         // Offset ones vector
                                    &beta,
                                    pdevData->d_VecAux, 1);

        assert(cuBlasStatus == CUBLAS_STATUS_SUCCESS && "Error in cuBlas calculation!");
    }
*/

void GPUCalcManager::calcAlphas()
{
    int m,n;
    cublasSideMode_t mode;
    
    mode=CUBLAS_SIDE_RIGHT;
    m = pdevData->nProt; //Cublas uses column-major notation, so we using this notation we can do our original operation
    n = pdevData->nReadsProcess;
    
    cuBlasStatus = cublasSdgmm(cuBlasHandle, mode,
                m, n,
                pdevData->d_MatAux, m,
                pdevData->d_VecAux, 1,
                pdevData->d_MatAux, m); //Documentation says that it is "in-place" if lda=ldc!
    
    assert(cuBlasStatus == CUBLAS_STATUS_SUCCESS && "Error in cuBlas calculation lib!4 Try reducing the number of reads on GPU by reducing memory usage.");
}

void GPUCalcManager::PXIRelSumRows()
{
    float alpha,beta;
    unsigned int m;
    cublasOperation_t trans;
    
    //Parameters fixing
    alpha=1.0f;beta=0; 
    trans=CUBLAS_OP_T; //Transpose  
    m=pdevData->nProt; //Cublas uses column-major notation, so we using this notation we can do our original operation
    //n=pdevData->nReadsProcess; //This would be the real size of the matrix, but we will batch it.
    

    //This operation is batched because gemv failed for very high N when using 152Prot!
    unsigned long batchSize=retrieveBatch(pdevData->nProt); 
    for (unsigned long j = 0; j < pdevData->nReadsProcess; j += batchSize) 
    {
        unsigned int current_n = std::min(batchSize, pdevData->nReadsProcess - j); //n of the matrix to process

        cuBlasStatus = cublasSgemv( cuBlasHandle, trans,
                                m, current_n,
                                &alpha,
                                pdevData->d_MatAux + (j*(unsigned long)m), m, 
                                pdevData->d_ones, 1,
                                &beta,
                                pdevData->d_VecAux + j, 1);
        assert(cuBlasStatus == CUBLAS_STATUS_SUCCESS && "Error in cuBlas calculation lib!3 Try reducing the number of reads on GPU by reducing memory usage.");
    }
    //Since we need 1/sum, we now invert the obtained vector with a custom kernel:
    unsigned int n_threads = pdevData->nReadsProcess;
    unsigned int n_blocks = (n_threads/NThreadsPerBlock)+1;
    invertVect<<<n_blocks,NThreadsPerBlock>>>(pdevData->d_VecAux,pdevData->nReadsProcess);
    cudaDeviceSynchronize();
}

void GPUCalcManager::calcPXIRel()
{
    int m,n;
    cublasSideMode_t mode;
    
    mode=CUBLAS_SIDE_LEFT;
    m = pdevData->nProt;
    n = pdevData->nReadsProcess;
    
    cuBlasStatus = cublasSdgmm(cuBlasHandle, mode,
                m, n,
                pdevData->d_MatAux, m,
                pdevData->d_PIEst, 1,
                pdevData->d_MatAux, m); //Documentation says that it is "in-place" if lda=ldc!
    assert(cuBlasStatus == CUBLAS_STATUS_SUCCESS && "Error in cuBlas calculation lib!2 Try reducing the number of reads on GPU by reducing memory usage.");
}


void GPUCalcManager::calcPXgIRel() //Assumes P_rem already calculated.
{ 
/* //Version with only a thread per read (I think is less efficient than the 2nd one!).
    unsigned int n_blocks= (devData->nReadsProcess/NThreadsPerBlock)+1;
    PIgXRelThreadPerRead<<<n_blocks,NThreadsPerBlock>>>(d_pdevData);
    */
    unsigned long n_threads = ((unsigned long)pdevData->nReadsProcess * (unsigned long)pdevData->nProt);
    unsigned long n_blocks = (n_threads/NThreadsPerBlock)+1;
    PIgXRelThreadPerReadPerProt<<<n_blocks,NThreadsPerBlock>>>(d_pdevData);
    
    cudaDeviceSynchronize();
}



void GPUCalcManager::calcPRem()
{
    //Variable declarations
    float alpha,beta;
    unsigned int m,n;
    cublasOperation_t trans;
    float norm_factor=1.0f/((float)pdevData->nFluExp); //To have Prem normalized dividing by number of flu exps.
    
    //Parameters fixing
    alpha=(-1)*norm_factor;beta=1*norm_factor; //Parameters for gemv
    trans=CUBLAS_OP_T; //Transpose to have column-major format of the transposed ( A is n_sparxn_reads)
    m=pdevData->nSparsity; //The matrix A is mxn but we use this representation + transpose because of cublas column-major format.
    n=pdevData->nReadsProcess;
    cudaMemcpy(pdevData->d_pRem, pdevData->d_ones, sizeof(float)*n, cudaMemcpyDeviceToDevice); //ones in beta, so we do 1-sum(ps).
    
    
    if(m==1) //easier calculation when one unique value is given
    {
        alpha= (-1);
        cuBlasStatus =  cublasSaxpy( cuBlasHandle, n,
                                &alpha,
                                pdevData->d_TopNFluExpScores, 1,
                                pdevData->d_pRem, 1);
        assert(cuBlasStatus == CUBLAS_STATUS_SUCCESS && "Error in cuBlas calculation lib! Try reducing the number of reads on GPU by reducing memory usage.");
        cuBlasStatus =  cublasSscal(cuBlasHandle, n,
                                &norm_factor,
                                pdevData->d_pRem, 1);
    }
    else//gemv: y= (alpha)*op(A)@x+ beta*y; where A is mxn matrix, x and y are vectors nx1. With the parameters set we get y=1-np.sum(topNFluExpScores,axis=1)
        cuBlasStatus = cublasSgemv( cuBlasHandle, trans,
                                m, n,
                                &alpha,
                                pdevData->d_TopNFluExpScores, m, 
                                pdevData->d_ones, 1,
                                &beta,
                                pdevData->d_pRem, 1);//lda is number of columns
                                
    assert(cuBlasStatus == CUBLAS_STATUS_SUCCESS && "Error in cuBlas calculation lib!1 Try reducing the number of reads on GPU by reducing memory usage.");
}


GPUCalcManager::~GPUCalcManager()
{
    cublasDestroy(cuBlasHandle);
}
