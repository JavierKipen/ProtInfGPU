#include "GPUCalcManager.h"
#include <iostream>
#include <vector>
#include <cassert>

using namespace std;

__global__ void PIgXRelThreadPerRead(DeviceData *d_devData);
__global__ void PIgXRelThreadPerReadPerProt(DeviceData *d_devData);
__global__ void invertVect(float * vec,unsigned int len);

//Kernel definitions
__global__ void PIgXRelThreadPerRead(DeviceData *d_devData)
{
    const unsigned int threadId = blockIdx.x*blockDim.x + threadIdx.x;
    float pRemRead,PCurrProtGivenReadRel;
    float * d_TopNFluExpScoresRead, *d_PIgXRelRead;
    unsigned int *d_TopNFluExpIdRead,FluExpIdOffset;
    
    if(threadId<d_devData->nReadsProcess)
    {
        pRemRead=d_devData->d_pRem[threadId];
        d_TopNFluExpScoresRead=&(d_devData->d_TopNFluExpScores[threadId*d_devData->nSparsity]); //Pointing towards current read flu scores
        d_TopNFluExpIdRead=&(d_devData->d_TopNFluExpId[threadId*d_devData->nSparsity]); //Pointing towards current read flu scores ids
        d_PIgXRelRead=&(d_devData->d_MatAux[threadId*d_devData->nProt]); //Pointing towards current read flu scores ids
        
        FluExpIdOffset=0; //Starts from the beggining of the fluexp array
        for(unsigned int currProt=0;currProt<d_devData->nProt;currProt++) //We will calculate P(I=CurrProt|X=readThreadId) (relative)
        {
            PCurrProtGivenReadRel=0;
            for(unsigned int currReadScoreId=0;currReadScoreId<d_devData->nSparsity;currReadScoreId++) //Sparse matrix calculation!
                for(unsigned int currFluExpOfProtId=FluExpIdOffset;currFluExpOfProtId<FluExpIdOffset+(d_devData->d_NFexpForI[currProt]);currFluExpOfProtId++) //For every possible flu exp that the given protein can produce:
                    if(d_TopNFluExpIdRead[currReadScoreId]== d_devData->d_FexpIdForI[currFluExpOfProtId]) //If the score Id could also be generated by the current protein
                        PCurrProtGivenReadRel += (d_TopNFluExpScoresRead[currReadScoreId] * d_devData->d_PFexpForI[currFluExpOfProtId]); //Accumulates prob
            PCurrProtGivenReadRel += pRemRead; //Adding normalizing error
            d_PIgXRelRead[currProt]=PCurrProtGivenReadRel;
            FluExpIdOffset+=d_devData->d_NFexpForI[currProt]; //Offset to get P(Fe|I=Currprot) and its indexes;
        }
    }
}

__global__ void invertVect(float * vec,unsigned int len){
    int tid = blockDim.x * blockIdx.x + threadIdx.x; //thread id;

    /* if valid, squre the array element */
    if (tid < len) 
        vec[tid] = (1/vec[tid]);
}

__global__ void PIgXRelThreadPerReadPerProt(DeviceData *d_devData)
{
    const unsigned long threadId = (unsigned long)blockIdx.x*(unsigned long)blockDim.x + (unsigned long)threadIdx.x;
    
    float pRemRead,PCurrProtGivenReadRel;
    float * d_TopNFluExpScoresRead, *d_PIgXRelRead;
    unsigned int *d_TopNFluExpIdRead,FluExpIdOffset;
    

    if( threadId< (d_devData->nReadsProcess*d_devData->nProt) ) //Only threads within the desired range.
    {
        const unsigned int currRead = threadId/d_devData->nProt; //gets currRead and currProt from thread Id (could still have mix within same block, but it is small)
        const unsigned int currProt = threadId%d_devData->nProt;
        
        pRemRead=d_devData->d_pRem[currRead];
        d_TopNFluExpScoresRead=&(d_devData->d_TopNFluExpScores[currRead*d_devData->nSparsity]); //Pointing towards current read flu scores
        d_TopNFluExpIdRead=&(d_devData->d_TopNFluExpId[currRead*d_devData->nSparsity]); //Pointing towards current read flu scores ids
        d_PIgXRelRead=&(d_devData->d_MatAux[(currRead*d_devData->nProt)+currProt]); //Pointing towards the point in the matrix to be calculated.
        
        //Getting the start of the flu exps prob for the curr protein
        FluExpIdOffset=0; //Starts from the beggining of the fluexp array
        for(unsigned int i=0;i<currProt;i++)
            FluExpIdOffset+=d_devData->d_NFexpForI[i];
       

        PCurrProtGivenReadRel=0;
        for(unsigned int currReadScoreId=0;currReadScoreId<d_devData->nSparsity;currReadScoreId++) //Sparse matrix calculation!
            for(unsigned int currFluExpOfProtId=FluExpIdOffset;currFluExpOfProtId<FluExpIdOffset+(d_devData->d_NFexpForI[currProt]);currFluExpOfProtId++) //For every possible flu exp that the given protein can produce:
                if(d_TopNFluExpIdRead[currReadScoreId]== d_devData->d_FexpIdForI[currFluExpOfProtId]) //If the score Id could also be generated by the current protein
                    PCurrProtGivenReadRel += (d_TopNFluExpScoresRead[currReadScoreId] * d_devData->d_PFexpForI[currFluExpOfProtId]); //Accumulates prob
        PCurrProtGivenReadRel += pRemRead; //Adding normalizing error
        *d_PIgXRelRead=PCurrProtGivenReadRel;
    }
}

bool checkNan(float *pArrayGPU, unsigned long len)
{
    bool retVal=false;
    vector<float> auxVec(len,0);
    cudaMemcpy(auxVec.data(), pArrayGPU, sizeof(float)*len, cudaMemcpyDeviceToHost); //The update is contained in the auxiliar vector.
    for(unsigned int i=0;i<len;i++)
    {
        if(isnan(auxVec[i]) || isinf(auxVec[i]))
        {
            cout << "Element " + to_string(i) + " is not a number." << endl;
            retVal=true;
        }    
    }
    return retVal;
}

bool checkZeroRows(float *pMatAux, unsigned long nProt,unsigned long nReads)
{
    bool retVal=false;
    unsigned long len = nReads*nProt;
    vector<float> auxVec(len,0);
    cudaMemcpy(auxVec.data(), pMatAux, sizeof(float)*len, cudaMemcpyDeviceToHost); //The update is contained in the auxiliar vector.
    for(unsigned int i=0;i<nReads;i++)
    {
        bool allReadZeros=true;
        for(unsigned int j=0;j<nProt;j++)
            if(auxVec[i*nProt+j]!=0)
                allReadZeros=false;
        if(allReadZeros)
            cout << "Row number: " << to_string(i) << " had all probs eq to zero."<< endl;
    }
    return retVal;
}

//Class functions

GPUCalcManager::GPUCalcManager()
{
    NThreadsPerBlock=DEFAULT_N_THREADS_PER_BLOCK;
}

void GPUCalcManager::init()
{
    cublasCreate(&cuBlasHandle);
    
}


void GPUCalcManager::calculateUpdate(DeviceData *pdevData, DeviceData *d_pdevData) //Whenever data is ready, this process is run to process the data contribution.
{
    this->pdevData=pdevData;
    this->d_pdevData=d_pdevData;
    calcPRem(); //Gets normalization factor of sparse matrix
    //checkNan(pdevData->d_pRem,pdevData->nReadsProcess);
    calcPXgIRel(); //PXgIRel is obtained
    
    //checkZeroRows(pdevData->d_MatAux, pdevData->nProt,pdevData->nReadsProcess);
    
    //checkNan(pdevData->d_MatAux,pdevData->nReadsProcess*pdevData->nProt);
    calcPXIRel(); //The joint relative matrix is normalized
    //checkNan(pdevData->d_MatAux,pdevData->nReadsProcess*pdevData->nProt);
    PXIRelSumRows(); //The sums over the the rows are calculated for normalization and alpha calc.
    //checkNan(pdevData->d_VecAux,pdevData->nReadsProcess);
    calcAlphas(); //Multiplies the normalized matrix with the 1/sum for normalizations
    //checkNan(pdevData->d_MatAux,pdevData->nReadsProcess*pdevData->nProt);
    sumAlphas(); //Sums all alphas through reads for the updates to p_I estimation!
}
void GPUCalcManager::sumAlphas()
{

    float alpha,beta;
    unsigned int m,n;
    cublasOperation_t trans;
    
    //Parameters fixing
    alpha=1;beta=0; 
    trans=CUBLAS_OP_N; //Transpose  
    m=pdevData->nProt; //Cublas uses column-major notation, so we using this notation we can do our original operation
    n=pdevData->nReadsProcess;
    
    

    cuBlasStatus = cublasSgemv( cuBlasHandle, trans,
                                m, n,
                                &alpha,
                                pdevData->d_MatAux, m, 
                                pdevData->d_ones, 1,
                                &beta,
                                pdevData->d_VecAux, 1);
    assert(cuBlasStatus == CUBLAS_STATUS_SUCCESS && "Error in cuBlas calculation lib! Try reducing the number of reads on GPU by reducing memory usage.");

}
void GPUCalcManager::calcAlphas()
{
    int m,n;
    cublasSideMode_t mode;
    
    mode=CUBLAS_SIDE_RIGHT;
    m = pdevData->nProt; //Cublas uses column-major notation, so we using this notation we can do our original operation
    n = pdevData->nReadsProcess;
    
    cuBlasStatus = cublasSdgmm(cuBlasHandle, mode,
                m, n,
                pdevData->d_MatAux, m,
                pdevData->d_VecAux, 1,
                pdevData->d_MatAux, m); //Documentation says that it is "in-place" if lda=ldc!
    
    assert(cuBlasStatus == CUBLAS_STATUS_SUCCESS && "Error in cuBlas calculation lib! Try reducing the number of reads on GPU by reducing memory usage.");
}

void GPUCalcManager::PXIRelSumRows()
{
    float alpha,beta;
    unsigned int m,n;
    cublasOperation_t trans;
    
    //Parameters fixing
    alpha=1.0f;beta=0; 
    trans=CUBLAS_OP_T; //Transpose  
    m=pdevData->nProt; //Cublas uses column-major notation, so we using this notation we can do our original operation
    n=pdevData->nReadsProcess;
    
    size_t free_mem, total_mem;
    cudaMemGetInfo(&free_mem, &total_mem);
    std::cout << "Free memory before call: " << free_mem / (1024 * 1024) << " MB\n";


    cuBlasStatus = cublasSgemv( cuBlasHandle, trans,
                                m, n,
                                &alpha,
                                pdevData->d_MatAux, m, 
                                pdevData->d_ones, 1,
                                &beta,
                                pdevData->d_VecAux, 1);
    assert(cuBlasStatus == CUBLAS_STATUS_SUCCESS && "Error in cuBlas calculation lib! Try reducing the number of reads on GPU by reducing memory usage.");
    //Since we need 1/sum, we now invert the obtained vector with a custom kernel:
    unsigned int n_threads = pdevData->nReadsProcess;
    unsigned int n_blocks = (n_threads/NThreadsPerBlock)+1;
    invertVect<<<n_blocks,NThreadsPerBlock>>>(pdevData->d_VecAux,pdevData->nReadsProcess);
    cudaDeviceSynchronize();
}

void GPUCalcManager::calcPXIRel()
{
    int m,n;
    cublasSideMode_t mode;
    
    mode=CUBLAS_SIDE_LEFT;
    m = pdevData->nProt;
    n = pdevData->nReadsProcess;
    
    cuBlasStatus = cublasSdgmm(cuBlasHandle, mode,
                m, n,
                pdevData->d_MatAux, m,
                pdevData->d_PIEst, 1,
                pdevData->d_MatAux, m); //Documentation says that it is "in-place" if lda=ldc!
    assert(cuBlasStatus == CUBLAS_STATUS_SUCCESS && "Error in cuBlas calculation lib! Try reducing the number of reads on GPU by reducing memory usage.");
}


void GPUCalcManager::calcPXgIRel() //Assumes P_rem already calculated.
{ 
/* //Version with only a thread per read (I think is less efficient than the 2nd one!).
    unsigned int n_blocks= (devData->nReadsProcess/NThreadsPerBlock)+1;
    PIgXRelThreadPerRead<<<n_blocks,NThreadsPerBlock>>>(d_pdevData);
    */
    unsigned int n_threads = pdevData->nReadsProcess*pdevData->nProt;
    unsigned int n_blocks = (n_threads/NThreadsPerBlock)+1;
    PIgXRelThreadPerReadPerProt<<<n_blocks,NThreadsPerBlock>>>(d_pdevData);
    
    cudaDeviceSynchronize();
}



void GPUCalcManager::calcPRem()
{
    //Variable declarations
    float alpha,beta;
    unsigned int m,n;
    cublasOperation_t trans;
    float norm_factor=1.0f/((float)pdevData->nFluExp); //To have Prem normalized dividing by number of flu exps.
    
    //Parameters fixing
    alpha=(-1)*norm_factor;beta=1*norm_factor; //Parameters for gemv
    trans=CUBLAS_OP_T; //Transpose to have column-major format of the transposed ( A is n_sparxn_reads)
    m=pdevData->nSparsity; //The matrix A is mxn but we use this representation + transpose because of cublas column-major format.
    n=pdevData->nReadsProcess;
    cudaMemcpy(pdevData->d_pRem, pdevData->d_ones, sizeof(float)*n, cudaMemcpyDeviceToDevice); //ones in beta, so we do 1-sum(ps).
    
    if(m==1) //easier calculation when one unique value is given
    {
        alpha= (-1);
        cuBlasStatus =  cublasSaxpy( cuBlasHandle, n,
                                &alpha,
                                pdevData->d_TopNFluExpScores, 1,
                                pdevData->d_pRem, 1);
        assert(cuBlasStatus == CUBLAS_STATUS_SUCCESS && "Error in cuBlas calculation lib! Try reducing the number of reads on GPU by reducing memory usage.");
        cuBlasStatus =  cublasSscal(cuBlasHandle, n,
                                &norm_factor,
                                pdevData->d_pRem, 1);
    }
    else//gemv: y= (alpha)*op(A)@x+ beta*y; where A is mxn matrix, x and y are vectors nx1. With the parameters set we get y=1-np.sum(topNFluExpScores,axis=1)
        cuBlasStatus = cublasSgemv( cuBlasHandle, trans,
                                m, n,
                                &alpha,
                                pdevData->d_TopNFluExpScores, m, 
                                pdevData->d_ones, 1,
                                &beta,
                                pdevData->d_pRem, 1);//lda is number of columns
    assert(cuBlasStatus == CUBLAS_STATUS_SUCCESS && "Error in cuBlas calculation lib! Try reducing the number of reads on GPU by reducing memory usage.");
}


GPUCalcManager::~GPUCalcManager()
{
    cublasDestroy(cuBlasHandle);
}
