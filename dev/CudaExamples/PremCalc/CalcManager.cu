#include "CalcManager.h"


using namespace std;

__global__ void PIgXRelThreadPerRead(DeviceDataPXgICalc *d_devData);
__global__ void PIgXRelThreadPerReadPerProt(DeviceDataPXgICalc *d_devData);

//Kernel definitions
__global__ void PIgXRelThreadPerRead(DeviceDataPXgICalc *d_devData)
{
    const unsigned int threadId = blockIdx.x*blockDim.x + threadIdx.x;
    float pRemRead,PCurrProtGivenReadRel;
    float * d_TopNFluExpScoresRead, *d_PIgXRelRead;
    unsigned int *d_TopNFluExpIdRead,FluExpIdOffset;
    
    if(threadId<d_devData->n_reads)
    {
        pRemRead=d_devData->d_pRem[threadId];
        d_TopNFluExpScoresRead=&(d_devData->d_TopNFluExpScores[threadId*d_devData->n_sparsity]); //Pointing towards current read flu scores
        d_TopNFluExpIdRead=&(d_devData->d_TopNFluExpId[threadId*d_devData->n_sparsity]); //Pointing towards current read flu scores ids
        d_PIgXRelRead=&(d_devData->d_MatAux[threadId*d_devData->n_prot]); //Pointing towards current read flu scores ids
        
        FluExpIdOffset=0; //Starts from the beggining of the fluexp array
        for(unsigned int currProt=0;currProt<d_devData->n_prot;currProt++) //We will calculate P(I=CurrProt|X=readThreadId) (relative)
        {
            PCurrProtGivenReadRel=0;
            for(unsigned int currReadScoreId=0;currReadScoreId<d_devData->n_sparsity;currReadScoreId++) //Sparse matrix calculation!
                for(unsigned int currFluExpOfProtId=FluExpIdOffset;currFluExpOfProtId<FluExpIdOffset+(d_devData->d_NFexpForI[currProt]);currFluExpOfProtId++) //For every possible flu exp that the given protein can produce:
                    if(d_TopNFluExpIdRead[currReadScoreId]== d_devData->d_FexpIdForI[currFluExpOfProtId]) //If the score Id could also be generated by the current protein
                        PCurrProtGivenReadRel += (d_TopNFluExpScoresRead[currReadScoreId] * d_devData->d_PFexpForI[currFluExpOfProtId]); //Accumulates prob
            PCurrProtGivenReadRel += pRemRead; //Adding normalizing error
            d_PIgXRelRead[currProt]=PCurrProtGivenReadRel;
            FluExpIdOffset+=d_devData->d_NFexpForI[currProt]; //Offset to get P(Fe|I=Currprot) and its indexes;
        }
    }
}

__global__ void PIgXRelThreadPerReadPerProt(DeviceDataPXgICalc *d_devData)
{
    const unsigned int threadId = blockIdx.x*blockDim.x + threadIdx.x;
    
    float pRemRead,PCurrProtGivenReadRel;
    float * d_TopNFluExpScoresRead, *d_PIgXRelRead;
    unsigned int *d_TopNFluExpIdRead,FluExpIdOffset;
    

    if(threadId<d_devData->n_reads*d_devData->n_prot) //Only threads within the desired range.
    {
        const unsigned int currRead = threadId/d_devData->n_prot; //gets currRead and currProt from thread Id (could still have mix within same block, but it is small)
        const unsigned int currProt = threadId%d_devData->n_prot;
        
        pRemRead=d_devData->d_pRem[currRead];
        d_TopNFluExpScoresRead=&(d_devData->d_TopNFluExpScores[currRead*d_devData->n_sparsity]); //Pointing towards current read flu scores
        d_TopNFluExpIdRead=&(d_devData->d_TopNFluExpId[currRead*d_devData->n_sparsity]); //Pointing towards current read flu scores ids
        d_PIgXRelRead=&(d_devData->d_MatAux[currRead*d_devData->n_prot+currProt]); //Pointing towards the point in the matrix to be calculated.
        
        //Getting the start of the flu exps prob for the curr protein
        FluExpIdOffset=0; //Starts from the beggining of the fluexp array
        for(unsigned int i=0;i<currProt;i++)
            FluExpIdOffset+=d_devData->d_NFexpForI[i];
       

        PCurrProtGivenReadRel=0;
        for(unsigned int currReadScoreId=0;currReadScoreId<d_devData->n_sparsity;currReadScoreId++) //Sparse matrix calculation!
            for(unsigned int currFluExpOfProtId=FluExpIdOffset;currFluExpOfProtId<FluExpIdOffset+(d_devData->d_NFexpForI[currProt]);currFluExpOfProtId++) //For every possible flu exp that the given protein can produce:
                if(d_TopNFluExpIdRead[currReadScoreId]== d_devData->d_FexpIdForI[currFluExpOfProtId]) //If the score Id could also be generated by the current protein
                    PCurrProtGivenReadRel += (d_TopNFluExpScoresRead[currReadScoreId] * d_devData->d_PFexpForI[currFluExpOfProtId]); //Accumulates prob
        PCurrProtGivenReadRel += pRemRead; //Adding normalizing error
        *d_PIgXRelRead=PCurrProtGivenReadRel;
    }
}


//Class functions

CalcManager::CalcManager()
{
    cublasCreate(&cuBlasHandle);
    NThreadsPerBlock=DEFAULT_N_THREADS_PER_BLOCK;
}

void CalcManager::setData(DeviceDataPXgICalc *devData,DeviceDataPXgICalc *d_devData)
{
    this->devData=devData;
    this->d_devData=d_devData;
}

void CalcManager::processReads() //Whenever data is ready, this process is run to process the data contribution.
{
    calcPRem(); //Gets normalization factor of sparse matrix
    calcPXgIRel(); //PXgIRel is obtained (eta(xi,j)).
    calcPXgI(); //The relative matrix is normalized
    calcAlphas(); //Multiplies the normalized matrix with the estimated prot probabilities
    sumAlphas(); //Gets the weight update for the given data.
}
void CalcManager::sumAlphas()
{
}
void CalcManager::calcAlphas()
{
}

void CalcManager::calcPXgIRel() //Assumes P_rem already calculated.
{ 
/*
    unsigned int n_blocks= (devData->n_reads/NThreadsPerBlock)+1;
    PIgXRelThreadPerRead<<<n_blocks,NThreadsPerBlock>>>(d_devData);
    */
    unsigned int n_threads = devData->n_reads*devData->n_prot;
    unsigned int n_blocks = (n_threads/NThreadsPerBlock)+1;
    PIgXRelThreadPerReadPerProt<<<n_blocks,NThreadsPerBlock>>>(d_devData);
    
    cudaDeviceSynchronize();
}


void CalcManager::calcPXgI()
{
 
}


void CalcManager::calcPRem()
{
    //Variable declarations
    float alpha,beta;
    unsigned int m,n;
    cublasOperation_t trans;
    float norm_factor=1.0f/((float)devData->n_flu_exp); //To have Prem normalized dividing by number of flu exps.
    
    //Parameters fixing
    alpha=(-1)*norm_factor;beta=1*norm_factor; //Parameters for gemv
    trans=CUBLAS_OP_T; //Transpose to have column-major format of the transposed ( A is n_sparxn_reads)
    m=devData->n_sparsity; //The matrix A is mxn but we use this representation + transpose because of cublas column-major format.
    n=devData->n_reads;
    cudaMemcpy(devData->d_pRem, devData->d_ones, sizeof(float)*n, cudaMemcpyDeviceToDevice); //ones in beta, so we do 1-sum(ps).
    
    
    //gemv: y= (alpha)*op(A)@x+ beta*y; where A is mxn matrix, x and y are vectors nx1. With the parameters set we get y=1-np.sum(topNFluExpScores,axis=1)
    cuBlasStatus = cublasSgemv( cuBlasHandle, trans,
                                m, n,
                                &alpha,
                                devData->d_TopNFluExpScores, m, 
                                devData->d_ones, 1,
                                &beta,
                                devData->d_pRem, 1);//lda is number of columns
}


CalcManager::~CalcManager()
{
    cublasDestroy(cuBlasHandle);
}
