#include "CalcManager.h"


using namespace std;

__global__ void PIgXRelThreadPerRead(DeviceDataPXgICalc *d_devData);
__global__ void PIgXRelThreadPerReadPerProt(DeviceDataPXgICalc *d_devData);
__global__ void invertVect(float * vec,unsigned int len);

//Kernel definitions
__global__ void PIgXRelThreadPerRead(DeviceDataPXgICalc *d_devData)
{
    const unsigned int threadId = blockIdx.x*blockDim.x + threadIdx.x;
    float pRemRead,PCurrProtGivenReadRel;
    float * d_TopNFluExpScoresRead, *d_PIgXRelRead;
    unsigned int *d_TopNFluExpIdRead,FluExpIdOffset;
    
    if(threadId<d_devData->n_reads)
    {
        pRemRead=d_devData->d_pRem[threadId];
        d_TopNFluExpScoresRead=&(d_devData->d_TopNFluExpScores[threadId*d_devData->n_sparsity]); //Pointing towards current read flu scores
        d_TopNFluExpIdRead=&(d_devData->d_TopNFluExpId[threadId*d_devData->n_sparsity]); //Pointing towards current read flu scores ids
        d_PIgXRelRead=&(d_devData->d_MatAux[threadId*d_devData->n_prot]); //Pointing towards current read flu scores ids
        
        FluExpIdOffset=0; //Starts from the beggining of the fluexp array
        for(unsigned int currProt=0;currProt<d_devData->n_prot;currProt++) //We will calculate P(I=CurrProt|X=readThreadId) (relative)
        {
            PCurrProtGivenReadRel=0;
            for(unsigned int currReadScoreId=0;currReadScoreId<d_devData->n_sparsity;currReadScoreId++) //Sparse matrix calculation!
                for(unsigned int currFluExpOfProtId=FluExpIdOffset;currFluExpOfProtId<FluExpIdOffset+(d_devData->d_NFexpForI[currProt]);currFluExpOfProtId++) //For every possible flu exp that the given protein can produce:
                    if(d_TopNFluExpIdRead[currReadScoreId]== d_devData->d_FexpIdForI[currFluExpOfProtId]) //If the score Id could also be generated by the current protein
                        PCurrProtGivenReadRel += (d_TopNFluExpScoresRead[currReadScoreId] * d_devData->d_PFexpForI[currFluExpOfProtId]); //Accumulates prob
            PCurrProtGivenReadRel += pRemRead; //Adding normalizing error
            d_PIgXRelRead[currProt]=PCurrProtGivenReadRel;
            FluExpIdOffset+=d_devData->d_NFexpForI[currProt]; //Offset to get P(Fe|I=Currprot) and its indexes;
        }
    }
}

__global__ void invertVect(float * vec,unsigned int len){
    int tid = blockDim.x * blockIdx.x + threadIdx.x; //thread id;

    /* if valid, squre the array element */
    if (tid < len) 
        vec[tid] = (1/vec[tid]);
}

__global__ void PIgXRelThreadPerReadPerProt(DeviceDataPXgICalc *d_devData)
{
    const unsigned int threadId = blockIdx.x*blockDim.x + threadIdx.x;
    
    float pRemRead,PCurrProtGivenReadRel;
    float * d_TopNFluExpScoresRead, *d_PIgXRelRead;
    unsigned int *d_TopNFluExpIdRead,FluExpIdOffset;
    

    if(threadId<d_devData->n_reads*d_devData->n_prot) //Only threads within the desired range.
    {
        const unsigned int currRead = threadId/d_devData->n_prot; //gets currRead and currProt from thread Id (could still have mix within same block, but it is small)
        const unsigned int currProt = threadId%d_devData->n_prot;
        
        pRemRead=d_devData->d_pRem[currRead];
        d_TopNFluExpScoresRead=&(d_devData->d_TopNFluExpScores[currRead*d_devData->n_sparsity]); //Pointing towards current read flu scores
        d_TopNFluExpIdRead=&(d_devData->d_TopNFluExpId[currRead*d_devData->n_sparsity]); //Pointing towards current read flu scores ids
        d_PIgXRelRead=&(d_devData->d_MatAux[currRead*d_devData->n_prot+currProt]); //Pointing towards the point in the matrix to be calculated.
        
        //Getting the start of the flu exps prob for the curr protein
        FluExpIdOffset=0; //Starts from the beggining of the fluexp array
        for(unsigned int i=0;i<currProt;i++)
            FluExpIdOffset+=d_devData->d_NFexpForI[i];
       

        PCurrProtGivenReadRel=0;
        for(unsigned int currReadScoreId=0;currReadScoreId<d_devData->n_sparsity;currReadScoreId++) //Sparse matrix calculation!
            for(unsigned int currFluExpOfProtId=FluExpIdOffset;currFluExpOfProtId<FluExpIdOffset+(d_devData->d_NFexpForI[currProt]);currFluExpOfProtId++) //For every possible flu exp that the given protein can produce:
                if(d_TopNFluExpIdRead[currReadScoreId]== d_devData->d_FexpIdForI[currFluExpOfProtId]) //If the score Id could also be generated by the current protein
                    PCurrProtGivenReadRel += (d_TopNFluExpScoresRead[currReadScoreId] * d_devData->d_PFexpForI[currFluExpOfProtId]); //Accumulates prob
        PCurrProtGivenReadRel += pRemRead; //Adding normalizing error
        *d_PIgXRelRead=PCurrProtGivenReadRel;
    }
}


//Class functions

CalcManager::CalcManager()
{
    cublasCreate(&cuBlasHandle);
    NThreadsPerBlock=DEFAULT_N_THREADS_PER_BLOCK;
}

void CalcManager::setData(DeviceDataPXgICalc *devData,DeviceDataPXgICalc *d_devData)
{
    this->devData=devData;
    this->d_devData=d_devData;
}

void CalcManager::processReads(float * outUpdateArray) //Whenever data is ready, this process is run to process the data contribution.
{
    calcPRem(); //Gets normalization factor of sparse matrix
    calcPXgIRel(); //PXgIRel is obtained
    calcPXIRel(); //The joint relative matrix is normalized
    PXIRelSumRows(); //The sums over the the rows are calculated for normalization and alpha calc.
    calcAlphas(); //Multiplies the normalized matrix with the 1/sum for normalizations
    sumAlphas(); //Sums all alphas through reads for the updates to p_I estimation!
    retrieveUpdate(outUpdateArray);
}
void CalcManager::retrieveUpdate(float * outUpdateArray)
{
    cudaMemcpy(outUpdateArray, devData->d_VecAux, sizeof(float)*devData->n_prot, cudaMemcpyDeviceToHost); //The update is contained in the auxiliar vector.
}
void CalcManager::sumAlphas()
{

    float alpha,beta;
    unsigned int m,n;
    cublasOperation_t trans;
    
    //Parameters fixing
    alpha=1;beta=0; 
    trans=CUBLAS_OP_N; //Transpose  
    m=devData->n_prot; //Cublas uses column-major notation, so we using this notation we can do our original operation
    n=devData->n_reads;
    
    

    cuBlasStatus = cublasSgemv( cuBlasHandle, trans,
                                m, n,
                                &alpha,
                                devData->d_MatAux, m, 
                                devData->d_ones, 1,
                                &beta,
                                devData->d_VecAux, 1);

}
void CalcManager::calcAlphas()
{
    int m,n;
    cublasSideMode_t mode;
    
    mode=CUBLAS_SIDE_RIGHT;
    m = devData->n_prot; //Cublas uses column-major notation, so we using this notation we can do our original operation
    n = devData->n_reads;
    
    cuBlasStatus = cublasSdgmm(cuBlasHandle, mode,
                m, n,
                devData->d_MatAux, m,
                devData->d_VecAux, 1,
                devData->d_MatAux, m); //Documentation says that it is "in-place" if lda=ldc!
}

void CalcManager::PXIRelSumRows()
{
    float alpha,beta;
    unsigned int m,n;
    cublasOperation_t trans;
    
    //Parameters fixing
    alpha=1.0f;beta=0; 
    trans=CUBLAS_OP_T; //Transpose  
    m=devData->n_prot; //Cublas uses column-major notation, so we using this notation we can do our original operation
    n=devData->n_reads;
    
    

    cuBlasStatus = cublasSgemv( cuBlasHandle, trans,
                                m, n,
                                &alpha,
                                devData->d_MatAux, m, 
                                devData->d_ones, 1,
                                &beta,
                                devData->d_VecAux, 1);
    //Since we need 1/sum, we now invert the obtained vector with a custom kernel:
    unsigned int n_threads = devData->n_reads;
    unsigned int n_blocks = (n_threads/NThreadsPerBlock)+1;
    invertVect<<<n_blocks,NThreadsPerBlock>>>(devData->d_VecAux,devData->n_reads);
    cudaDeviceSynchronize();
}

void CalcManager::calcPXIRel()
{
    int m,n;
    cublasSideMode_t mode;
    
    mode=CUBLAS_SIDE_LEFT;
    m = devData->n_prot;
    n = devData->n_reads;
    
    cuBlasStatus = cublasSdgmm(cuBlasHandle, mode,
                m, n,
                devData->d_MatAux, m,
                devData->d_PIEst, 1,
                devData->d_MatAux, m); //Documentation says that it is "in-place" if lda=ldc!
}


void CalcManager::calcPXgIRel() //Assumes P_rem already calculated.
{ 
/* //Version with only a thread per read (I think is less efficient than the 2nd one!).
    unsigned int n_blocks= (devData->n_reads/NThreadsPerBlock)+1;
    PIgXRelThreadPerRead<<<n_blocks,NThreadsPerBlock>>>(d_devData);
    */
    unsigned int n_threads = devData->n_reads*devData->n_prot;
    unsigned int n_blocks = (n_threads/NThreadsPerBlock)+1;
    PIgXRelThreadPerReadPerProt<<<n_blocks,NThreadsPerBlock>>>(d_devData);
    
    cudaDeviceSynchronize();
}



void CalcManager::calcPRem()
{
    //Variable declarations
    float alpha,beta;
    unsigned int m,n;
    cublasOperation_t trans;
    float norm_factor=1.0f/((float)devData->n_flu_exp); //To have Prem normalized dividing by number of flu exps.
    
    //Parameters fixing
    alpha=(-1)*norm_factor;beta=1*norm_factor; //Parameters for gemv
    trans=CUBLAS_OP_T; //Transpose to have column-major format of the transposed ( A is n_sparxn_reads)
    m=devData->n_sparsity; //The matrix A is mxn but we use this representation + transpose because of cublas column-major format.
    n=devData->n_reads;
    cudaMemcpy(devData->d_pRem, devData->d_ones, sizeof(float)*n, cudaMemcpyDeviceToDevice); //ones in beta, so we do 1-sum(ps).
    
    
    //gemv: y= (alpha)*op(A)@x+ beta*y; where A is mxn matrix, x and y are vectors nx1. With the parameters set we get y=1-np.sum(topNFluExpScores,axis=1)
    cuBlasStatus = cublasSgemv( cuBlasHandle, trans,
                                m, n,
                                &alpha,
                                devData->d_TopNFluExpScores, m, 
                                devData->d_ones, 1,
                                &beta,
                                devData->d_pRem, 1);//lda is number of columns
}


CalcManager::~CalcManager()
{
    cublasDestroy(cuBlasHandle);
}
