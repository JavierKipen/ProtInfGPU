We are going to push the calculation of the EM. We have two cases that we divide


***************Oracle (Nsparsity=1):****************************

We can do a special kernels with an if Nsparsity=1. Here we ran a simulation with 1 million reads on proteome and we get the computational times of kernel vs whole update calculation:

Calc total time: 36331.4ms. Kernel time: 36209.6ms
Calc total time: 36137.1ms. Kernel time: 36015.3ms
Calc total time: 20371.2ms. Kernel time: 20302.5ms
Calc total time: 36292.4ms. Kernel time: 36170.6ms
Calc total time: 36136.4ms. Kernel time: 36014.6ms
Calc total time: 20371.4ms. Kernel time: 20302.6ms
Calc total time: 36292ms. Kernel time: 36170.3ms
Calc total time: 36136.1ms. Kernel time: 36014.3ms
Calc total time: 20371.3ms. Kernel time: 20302.6ms


With 2 million, it partitions the data in 3 sets, thats why the third is always shorter. But in general we get that the kernel is >300 times slower than the rest of calculations. Optimizing the kernel will lead to improved computing times. One rep takes around 36 seconds!.

After reading a bit i think that I have to optimize memory usage. I can use shared memory for storing the P(Fe|I), and have many proteins in each block for many reads -> if they access similar memory places would be less races. Easiest way to group is to put in order number of proteins and select a few, but I have to be careful to pick how many proteins so it fits in the shared memory. In shared memory online says 48 kB -> 6144 entries. Then grouping by N in python we get:
194 proteins together overpasses that limit. Lets keep it below that and try without any reordering. 

Timing from 364853 ms to 4668.35 ms. BUT THIS WAS WITH DEBUG INFO. with -O3 compilation goes from 36037.2 ms to 4562.46 ms.

And if we only copy the entries without prem? goes down to 279.235 ms .

We first check the kernel with calculation of each item!

Things to verify:

- New PXgIRel does not have nans -> No NaN!
- New PXgIRel does not have rows with whole zeroes! -> no Zero!
- PXgIRel is equal to the one generated with our previous kernel -> no Zero

Lets go further. In the case of 279 is fast, but copying with a simple kernal the contriubtion of prem was still 4 seconds. I suspect is because of how memory is accesed. Will try to coordinate it better. Now it was 89ms, so it seems to work like that.

I checked the copy of prem coordinated, and now it seems to be working. Lets see if it is the same output now as the other kernel!
It seems to be the same now!! Great. We checked the sum of whole matrix and the first 1000 reads of matAux are equal. 

I think with this we finished the optimization of Oracle, now we will try to add to the main code and see if it works correctly.

***************Non oracle ****************************

nvcc main.cpp GPUDataManager.cu GPUKernelManager.cu IOManager.cpp Wrapper.cpp -g

Base timing:
1.30502e+06 ms -> 21 mins to calculate only one update!!!!!!!!

The sum of the whole matrix is 66754.125. 
